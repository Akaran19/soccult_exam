Course,Book,Chapter,Author,Summary
Introduction to cognitive science,Students guide to neuroscience,The social and emotional brain,Ward J.,"Although there are many different theories of emotion (some in vogue, some rejected), there are a core set of ideas concerning emotions that have stood the test of time. This includes the idea that emotions have an evolved adaptive value, and this is largely conserved across species. It also includes the notion that emotions are multi-faceted: they contain both conscious (at least in humans) and unconscious processes; they involve the interplay of brain and body via the autonomic system (although emotions cannot be reduced to bodily sensations) and that (at least in humans) some emotions are constructed from both affective mechanisms and cognitive ones (e.g. appraisal). A good example of the latter is the so-called moral emotions (e.g. guilt, pride). Contemporary theories emphasize categorical distinctions between emotions (such as anger, fear sadness) but differ with regards to whether these categories represent natural kinds (i.e. innately specified categorical differences, as in the basic emotion approach) or are themselves constructed from different combinations of building blocks of other kinds of core processes (e.g. reward/punishment, pleasure, arousal, appraisals)."
Introduction to cognitive science,Students guide to neuroscience,The social and emotional brain,Ward J.,"This section has outlined a set of regions that are critically involved in the processing of emotions. In social animals, such as humans, these emotional brain regions play a key role in evaluating and judging social stimuli. For instance, the amygdala is not only implicated in evaluating whether a tone will lead to a shock, but also in evaluating whether another person is afraid; the anterior cingulate responds not only to physical pain but also responds to social pain relating to separation and social exclusion; and the nucleus accumbens responds not only to basic rewards (food, sex) but also responds when we opt to cooperate with another person. The different regions of the emotional brain serve different functions, and this is at odds with earlier theories of emotion (e.g. the Papez circuit and Maclean’s “limbic brain”). However, there is not a simple one-to-one mapping between brain structure and emotional category (e.g. amygdala = fear, insula = disgust) as predicted by a strong version of the “basic emotion” approach. Of course, “basic emotions” could still be said to exist at the level of brain circuits connecting specialized sub-regions and the best documented examples in the literature are fear and disgust. Other contemporary theories postulate the notion of “core affect” which consists of perceived internal body states and organized around the dimensions of intensity and pleasantness (Feldman Barrett, 2006). While there are brain regions that appear to have these characteristics (e.g. the amygdala shows some evidence of tracking intensity) on closer inspection their functioning is far more complex (e.g. the evidence that the amygdala has somewhat different roles in fear versus reward conditioning). Finally, almost all contemporary theories of emotion allow for a role of ""cognition"" by which is meant thoughts and beliefs, appraisals, control mechanisms and the like. This is supported by the neuroscience evidence but stands in contrast to some earlier views that emotion was somehow distinct from cognition (for a summary see Phelps, 2006). An emotion is best regarded as a complex affective state in which cognitions are intrinsically embedded rather than standing apart."
Introduction to cognitive science,Cognitive psychology and its implications,Perception,Anderson J. R.,"This chapter discusses how neurons process sensory information and deliver it to higher centers in the brain, and how the information is then processed and combined with contextual information to perceive and recognize objects. Figure 2.31 depicts the overall flow of information processing in the case of vision perception. Perception begins with light energy from the external environment. Receptors, such as those in the retina, transform this energy into neural information. Early sensory processing makes initial sense of the information by extracting features to yield what Marr (1982) called a primal sketch. These features are combined with depth information to get a representation of the location of surfaces in space; this is Marr’s 2½-D sketch. The gestalt principles of organization are applied to segment the elements in the 2½-D sketch into objects; this is Marr’s 3-D model. Finally, information about the features of these objects and general context information are combined to recognize the objects. The output of this last level is a representation of the objects and their locations in the environment, and this is what we are consciously aware of in perception. This information is then input to higher-level cognitive processes. Figure 2.31 illustrates an important point: A great deal of information processing must take place before we are consciously aware of the objects we are perceiving."
Introduction to cognitive science,Cognitive psychology and its implications,Attention and Performance,Anderson J. R.,"There has been a gradual shift in the way cognitive psychology perceives the issue of attention. For a long time, the implicit assumption was captured by this famous quote from William James (1890, pp. 403–404) over a century ago: Everyone knows what attention is. It is the taking possession by the mind, in a clear and vivid form, of one out of what seem several simultaneously possible objects or trains of thought. Focalization, concentration of consciousness are of its essence. It implies withdrawal from some things in order to deal effectively with others. Two features of this quote reflect conceptions once held about attention. The first is that attention is strongly related to consciousness — we cannot attend to something unless we are conscious of it. The second is that attention, like consciousness, is a unitary system. More and more, cognitive psychology is beginning to recognize that attention also operates at an unconscious level. For instance, people often are not conscious of where they have moved their eyes. Along with this recognition has come the realization that attention is multifaceted (e.g., Chun, Golumb, & Turk-Browne, 2011). We have seen that it makes sense to separate auditory attention from visual attention and to distinguish between attention in perceptual processing, attention in executive control, and attention in response generation. The brain consists of a number of parallel processing systems for the various perceptual systems, for motor systems, and for central cognition. Each of these parallel systems seems to suffer bottlenecks — points at which it must focus its processing on a single thing. Attention is best conceived as the processes by which each of these systems is allocated to potentially competing information-processing demands. The amount of interference that occurs among tasks making demands on the same system is a function of the overlap in timing among those demands."
Introduction to cognitive science,Cognitive psychology and its implications,Mental Imagery,Anderson J. R.,"This chapter has reviewed some of the evidence that the brain regions involved in visual perception are also involved in visual imagery. Such research has presumably put to rest the question raised at the beginning of the chapter about whether visual imagery really has a perceptual character. However, although it seems clear that perceptual processes are involved in visual imagery to some degree, the degree to which the mechanisms of visual imagery are the same as the mechanisms of visual perception remains an open question. Evidence for a substantial overlap comes from neuropsychological patient studies (see Bartolomeo, 2002, for a review). Many patients who have cortical damage leading to blindness have corresponding deficits in visual imagery. As Behrmann (2000) notes, the correspondences between perception and imagery can be quite striking. For instance, there are patients who are not able to perceive or image faces and colors, but are otherwise unimpaired in either perception or imagery. Nonetheless, there also exist cases of patients who suffer perceptual problems but have intact visual imagery, and vice versa. Behrmann argues that visual perception and visual imagery are best understood as two processes that overlap but are not identical, as illustrated in Figure 4.22. Perceiving a kangaroo requires low-level visual information processing that is not required for visual imagery. Similarly, forming a mental image of a kangaroo requires high-level generation processes that are not required by perception. Behrmann suggests that patients who suffer only perceptual losses have damage to the low-level part of this system, and patients who suffer only imagery losses have damage to the high-level part of this system."
Introduction to cognitive science,Cognitive psychology and its implications,Representation of Knowledge,Anderson J. R.,"Estimates of the storage capacity (e.g., Moll & Miikkulainen, 1997; Treves & Rolls, 1994) of the brain differ substantially, but they are all many orders of magnitude less than what would be required to store a faithful video recording of our whole life. This chapter has reviewed studies of what we retain and what we forget — for instance, for Figure 5.5, what subject was being taught, but not what the teacher was wearing; or for Figure 5.14, that the room was an office, but not what was in the office. The chapter also reviewed three perspectives on the basis for this selective memory. 1. The multimodal hypothesis (Figure 5.12 left), which proposes that we select important aspects of our experience to remember and often convert meanings from one modality to another. For instance, we may describe a room (visual) as an “office” (verbal). 2. The amodal hypothesis (Figure 5.12 right), which proposes that we convert our experience into some abstract representation that just encodes what is important. For instance, the chapter discussed how propositions captured the connections among the concepts in our understanding of a sentence. 3. Theories proposing that we remember our experiences in terms of the categories that they seem to exemplify. We discussed a varied set of such category representations: semantic networks, schemas, scripts, exemplar, prototype, and rule based. These hypotheses are not mutually exclusive, and cognitive scientists are actively engaged in trying to understand when a particular kind of knowledge representation is used to encode experience."
Introduction to cognitive science,Cognitive psychology and its implications,Human Memory: Encoding and Storage,Anderson J. R.,"This chapter has focused on the processes involved in getting information into memory. We saw that a great deal of information gets registered in sensory memory, but relatively little can be maintained in short-term or working memory and even less survives for long periods of time. However, an analysis of what actually gets stored in long-term memory really needs to consider how that information is retained and retrieved — which is the topic of the next chapter. Many of the issues considered in this chapter are complicated by retrieval issues. This is certainly true for the effects of elaborative processing that we have just discussed. There are important interactions between how a memory is processed at study and how it is processed at test. Even in this chapter, we were not able to discuss the effects of such factors as practice without discussing the activation-based retrieval processes that are facilitated by these factors. Chapter 7 will also have more to say about the activation of memories."
Introduction to cognitive science,Cognitive psychology and its implications,Human Memory: Retention and Retrieval,Anderson J. R.,"Figure 7.17 shows the different types and subtypes of memory proposed by Squire (1987). The major distinction in his classification is between explicit memory and implicit memory, which he calls declarative memory and nondeclarative memory. It appears that the hippocampus is particularly important for the establishment of declarative memories. Within the declarative memory system, there is a distinction between episodic and semantic memory. Episodic memories include information about where and when they were acquired. For example, a memory of a particular newscast can be considered an episodic memory. This chapter and Chapter 6 have discussed these kinds of memories. Semantic memories, discussed in Chapter 5, reflect general knowledge of the world, such as what a dog is or what a restaurant is. Figure 7.17 makes it clear that there are many kinds of nondeclarative, or implicit, memories. We have just completed a discussion of procedural memories and the critical role of the basal ganglia and cerebellum in their formation. We also talked about priming and the fact that priming seems to entail changes to cortical regions directly responsible for processing the information involved. There are other kinds of learning that we have not discussed but that are particularly important in studies of animal learning. These include conditioning and the two types of memory labeled nonassociative in Figure 7.17, habituation and sensitization, all of which have been demonstrated in species ranging from sea slugs to humans. Evidence suggests that conditioning in mammals involves many different brain structures (Anderson, 2000), as do many other types of learning, with different brain structures supporting different kinds of learning."
Introduction to cognitive science,Cognitive psychology and its implications,Problem Solving,Anderson J. R.,"This chapter has been built around the Newell and Simon model of problem solving as a search through a state space defined by operators. We have looked at problem-solving success as determined by the operators available and the methods used to guide the search for operators. This analysis is particularly appropriate for first-time problems, whether a chimpanzee’s quandary (see Figure 8.2) or a human’s predicament when shown a Tower of Hanoi problem for the first time (see Figure 8.10). The next chapter will focus on the other factors that come into play after repeated practice at problem solving."
Introduction to cognitive science,Cognitive psychology and its implications,Expertise,Anderson J. R.,"This chapter began by noting the remarkable ability of humans to acquire the complexities of culture and technology. In fact, in today’s world people routinely acquire whole new sets of skills over their lifetimes. For instance, I now use my phone for instant messaging, GPS navigation, and surfing the Web — functions that I could not have imagined when I was a young man, let alone associated with a phone. This chapter has emphasized the role of practice in acquiring such skills, and certainly it has taken me some considerable practice to master them. However, human flexibility depends on more than time on task — other creatures could never acquire such skills. Critical to human expertise are the higher-order problem-solving skills that we reviewed in Chapter 8. Also critical is our human ability to reason, make decisions, and communicate by language. These are the topics of the forthcoming chapters."
Introduction to cognitive science,Cognitive psychology and its implications,Decision Making,Anderson J. R.,"Decision making deals with choosing actions that can have real consequences in the presence of real uncertainty. All mammals have the dopamine system that we just described, which gives them a basic ability to seek things that are rewarding and avoid things that are harmful. However, humans, by virtue of their greatly expanded prefrontal cortex, have a highly developed capacity to reflect on their circumstances and select actions other than what their more primitive systems might urge. Research suggests that the ventromedial portion of the human prefrontal cortex, which is proportionally much larger than the same region in the genetically similar apes, might play a particularly important role in such regulation. Humans attempt acts of self-regulation — for example, diet plans — that are far beyond the reach of any other species. However, we live in an uncertain world, as evidenced by all the contradictory claims made for various diet plans. Perhaps if we understood better how people respond to such uncertainty and to such contradictory information, we would also be in a better position to understand why there are so many failures of our good resolutions."
Introduction to cognitive science,Cognitive psychology and its implications,Individual differences in Cognition,Anderson J. R.,"Two recurring themes throughout this book have been the diversity of the components of the mind and the specialization of different brain regions to perform different functions. The first chapter reviewed the evidence for different specializations in the nervous system. Subsequent chapters reviewed the evidence for different levels of processing as information entered the system and then discussed the different types of knowledge representation and the distinction between procedural and declarative knowledge. Then, we considered the distinct status of language. Many of the findings discussed in those chapters have been reinforced in this chapter on individual differences. Another recurring theme has been the importance of the rate of processing: Response time has been the most frequently used measure of cognitive functioning in this book. Often, error measures (the second most common dependent measure) were shown to be merely indications of slow processing. We have seen evidence in this chapter that individuals vary in their rate of processing, and we have stressed that this rate can be increased with practice. In addition to the quantitative component of speed, individual differences have a qualitative component: People can differ in where their strengths lie. They can also differ in their selection of strategies for solving problems. We saw evidence in Chapter 9 that one dimension of growing expertise is the development of more effective strategies. One might view the human mind as being analogous to a large corporation that consists of many interacting components. The differences among corporations are often due to the relative strengths of their components. With practice, different components tend to become more efficient at doing their tasks. Another way to achieve improvement is by strategic reorganizations of parts of the corporation. However, there is more to a successful company than just the sum of its parts: The parts have to interact smoothly to achieve the overall goals of the organization. Some researchers (e.g., Newell, 1990) have complained about the rather fragmented picture of the human mind that emerges from current research in cognitive psychology. One agenda for future research will be to understand how all the pieces fit together to make a human mind."
Methods,Statistics for linguists: An Introduction using R,Introduction to R,Winter B.,"R is a statistical programming language that is ideal for analyzing linguistic data. This chapter provides the first stepping stones in writing an R script, including how to work with functions and arguments, as well as with different vector classes. The chapter also introduces the reader to assigning variables in R, and to ways of indexing/subsetting existing R objects. In addition, readers will be introduced to basic logical operations, matrices, data frames, file management, installing and citing R packages, and plotting. After a first look at a cross-linguistic dataset of how linguistic diversity is affected by ecological risk, the chapter concludes with recommendations for how to proceed learning more about R."
Methods,Statistics for linguists: An Introduction using R,The Tidyverse and Reproducible Workflows,Winter B.,"The “tidyverse” is a network of packages that makes working with R much easier. Among other things, “tidyverse”-style functions (such as from the “dplyr” and “ggplot2” packages) make it much easier to write clean code, which facilitates the “reproducibility” of one’s analysis, i.e., the ability of other researchers to recreate every step in the data analysis cycle. Besides teaching how to wrangle with data, this chapter introduces reproducible research practices, including R Markdown. The chapter also addresses common objections against sharing one’s data and code."
Methods,Statistics for linguists: An Introduction using R,"Descriptive statistics, Models, and Distribution",Winter B.,"Statistics is the process of making complex data structures amenable to human cognition. The chapter introduces the mean, a measure of the central tendency of a distribution. The mean is discussed as a “model”, in the sense that it provides a simplified representation of a set of numbers that is easier to grasp than the entire set of numbers. Another important “model” is the standard deviation, which measures the spread of a distribution. Alongside a discussion of these basic descriptive statistics, the uniform distribution and normal distribution (Gaussian) are introduced to the reader, together with the important fundamental distinction between sample estimates and population parameters. The chapter concludes with a descriptive analysis of how “good” or “bad” certain words are (emotional valence)."
Methods,Statistics for linguists: An Introduction using R,"Correlation, Linear, and Nonlinear transformations",Winter B.,"When researchers, including linguists, use linear models to analyze their data, they often need to “transform” the variables in their study. This chapter discusses linear and nonlinear transformations. Whereas linear transformations have rather innocuous effects (they leave the relationships between data points untouched), nonlinear transformations can substantially alter one’s conclusions (the relationships between data points is affected). The chapter discusses “centering” (subtracting the mean) and “standardizing” (dividing by the standard deviation) as two common linear transformations. The main nonlinear transformation discussed here is the logarithm, which is ubiquitous in linguistics. Extensive examples tell the reader how to interpret linear models when variables have been transformed. The chapter concludes with a discussion of correlation as it can be thought of as a standardized form of regression."
Methods,Statistics for linguists: An Introduction using R,Inferential statistics 1: Significance Testing,Winter B.,"Inferential statistics describes the process of making inferences onto a population of interest (such as the population of all English speakers) based on a sample that only represents a subset of this population (such as a small group of English speakers that were tested in a linguistic study). This chapter looks at how certainty in one’s sample estimates can be quantified, so as to develop a procedure (null hypothesis significance testing) that gives us a clear decision procedure for making claims about the population. Along the way, the chapter also introduces the very important concept of “effect size”, and standardized effect size measures such as Pearson’s correlation coefficient and Cohen’s d are discussed. After discussing effect size, the reader is introduced to standard errors and confidence intervals. The chapter concludes with a discussion of the t-test for comparing two group means (such as testing whether men and women differ in voice pitch), which leads to a discussion of p-values."
Methods,Statistics for linguists: An Introduction using R,Inferential statistics 2: Issues with Significance Testing,Winter B.,"Null hypothesis significance testing is not without its issues, and misapplications abound in the literature. This chapter discusses common misinterpretations of p-values, followed by a discussion of things that can go wrong: Type I errors (false positives), Type II errors (false negatives), Type M errors (magnitude errors), and Type S errors (sign errors). This discussion is used to argue for the importance of statistical power (the ability to detect an effect if the null hypothesis is actually false). Researchers in all disciplines, including linguistics, should strive to conduct “high-powered” studies—for example, by collecting as much data as possible. The chapter also discusses the important issue of “multiple testing”: when a researcher conducts lots of significance tests on the same dataset, the chance that any of these tests is significant increases drastically—even if nothing is going on in the dataset. This issue can be circumvented by “correcting” one’s alpha level (being more conservative in accepting significant results); however, the best recommendation is to perform as few significance tests if possible—especially if there are clear theoretical motivations for doing so."
Methods,Statistics for linguists: An Introduction using R,Introduction to Linear model: Simple Linear Regression,Winter B.,"Linear models are ubiquitous in statistics and can be found all across the language sciences. This chapter introduces the reader to simple linear regression (ordinary least squares regression) using a psycholinguistic dataset as an example (word frequency effects). Basic linear regression concepts are discussed, such as the distinction between intercepts and slopes, the distinction between fitted values and residuals, and the concept of error minimization (“best fitting line”). The chapter also discusses R 2 as a measure of how much variance is described by a linear model."
Methods,Statistics for linguists: An Introduction using R,Mulitple Regression,Winter B.,"Linear models become much more interesting once multiple predictors are included (“multiple regression”). This chapter provides an extensive example of a dataset on iconicity (the degree to which a word sounds like what it means, such as onomatopeic forms like “beeping” and “squealing”). Multiple predictors are included into a linear model to look at how iconicity is affected by various word-level properties. The chapter also clearly shows why it is important to standardize one’s variables if the comparison of the relative impact of different predictors is of interest. The chapter concludes with a discussion of linear modeling assumptions, including normality and homoscedasticity (“constant variance”/“equal variance”)."
Methods,Statistics for linguists: An Introduction using R,Categorical Predictors,Winter B.,"One of the true powers of the linear model framework is that predictors can be both continuous (such as pitch, which is measured on the continuous scale of Hertz) or categorical (such as active versus passive voice, or male versus female sex). This chapter introduces the reader to how categorical predictors can be incorporated a linear model analysis. Two “coding schemes” for categorical predictors are discussed in this chapter, including “treatment coding” (R’s default) and “sum-coding”. In a hands-on example, the reader is guided through an analysis of the emotional properties of taste and smell words (such as sweet, bitter, pungent, and fragrant)."
Methods,Statistics for linguists: An Introduction using R,Mixed models 1: Conceptual Introduction,Winter B.,"Mixed models (aka multilevel models) are incredibly important when dealing with situations where there are clusters of non-independent data points, such as is the case with almost all linguistic experiments (“repeated measures experiments”, in particular in psycholinguistics, sociolinguistics, and phonetics). This chapter provides a conceptual introduction to mixed models from the perspective of the “independence assumption” of regression modeling, according to which each data point has to be independent. When this is violated, mixed models can be used to resolve the violation and inform the model about sources of heterogeneity in the data. The reader is introduced to the distinction between fixed and random effects, as well as to the distinction between random intercepts and random slopes. The chapter also introduces the R syntax used to specify mixed models in the widely used lme4 package."
Methods,Statistics for linguists: An Introduction using R,"Mixed Models 2: Extended Example, Significance Testing, Convergence Issues",Winter B.,"This chapter delves more deeply into mixed models and provides two hands-on examples. First, a dataset is constructed using random number generation functions. This dataset is then analyzed with mixed models. Because the dataset is hand-generated, the “ground truth” of the data can be compared to the model output. The reader is walked through interpreting the output of a mixed model. The second example introduces mixed logistic regression, a form of mixed model suitable for binary categorical data. In this case, a discourse analysis dataset of “ugly selfies” is analyzed. The chapter also discusses remaining topics in mixed models, such as convergence issues and shrinkage."
Methods,Statistics for linguists: An Introduction using R,Generalized Linear Models 1: Logistic Regression,Winter B.,"In a lot of linguistic applications, researchers are interested in modeling a categorical outcome variable (such as whether a speaker has used active voice or passive voice) as a function of various predictors. For this, generalized linear models (GLMs) can be used, which are an extension of linear models. With GLMs, one can relax assumptions imposed on the distribution of the response variable, which allows modeling categorical data. The reader is introduced to one specific form of GLM, logistic regression. This form of regression is suitable to modeling binary categorical data (active versus passive voice, correct versus incorrect, etc.). Along the way, the chapter introduces the Bernoulli distribution, log odds (logits), and the logistic function. Since many errors can happen with interpreting logit coefficients, the chapter walks the reader through several examples, including modeling the English dative alternation and a psycholinguistic experiment of gesture perception."
Methods,Statistics for linguists: An Introduction using R,Interactions and Nonlinear Effects,Winter B.,"The term “interaction” describes situations in which one predictor’s influence on a linguistic variable is conditioned on another predictor. Informally, we can speak of interactions as cases where predictors are “more than the sum of their parts”. Mathematically, interactions are multiplicative rather than additive effects. Interactions are extremely common in linguistic data, and they are often theoretically very interesting. However, unfortunately, interactions make the interpretation of one’s models much more difficult, and many linguists commonly misinterpret their models in the presence of interactions. To combat this, the chapter walks the reader through numerous examples of interactions. Example analyses include datasets of a conceptual metaphor experiment, as well as a study on iconicity (how much a word sounds like what it means)."
Methods,Discovering Statistics Using R,Exploring Assumptions,Field A.,"‘You promised us swans,’ I hear you cry, ‘and all we got was normality this, homosomethingorother that, transform this, it’s all a waste of time that. Where were the bloody swans?!’ Well, the Queen owns them all so I wasn’t allowed to have them. Nevertheless, this chapter did negotiate Dante’s eighth circle of hell (Malebolge), where data of deliberate and knowing evil dwell. That is, data that don’t conform to all of those pesky assumptions that make statistical tests work properly. We began by seeing what assumptions need to be met for parametric tests to work, but we mainly focused on the assumptions of normality and homogeneity of variance. To look for normality we rediscovered the joys of frequency distributions, but also encountered some other graphs that tell us about deviations from normality (Q-Q plots). We saw how we can use skew and kurtosis values to assess normality and that there are statistical tests that we can use (the Shapiro–Wilk test). While negotiating these evildoers, we discovered what homogeneity of variance is, and how to test it with Levene’s test and Hartley’s Fmax. Finally, we discovered redemption for our data. We saw we can cure their sins, make them good, with transformations (and on the way we discovered some of the uses of the by() function and the transformation functions). Sadly, we also saw that some data are destined always to be evil. We also discovered that I had started to read. However, reading was not my true passion; it was music. One of my earliest memories is of listening to my dad’s rock and soul records (back in the days of vinyl) while waiting for my older brother to come home from school, so I must have been about 3 at the time. The first record I asked my parents to buy me was ‘Take on the World’ by Judas Priest, which I’d heard on Top of the Pops (a now defunct UK TV show) and liked. This record came out in 1978 when I was 5. Some people think that this sort of music corrupts young minds. Let’s see if it did …"
Methods,Discovering Statistics Using R,Correlation,Field A.,"This chapter has looked at ways to study relationships between variables. We began by looking at how we might measure relationships statistically by developing what we already know about variance (from Chapter 1) to look at variance shared between variables. This shared variance is known as covariance. We then discovered that when data are parametric we can measure the strength of a relationship using Pearson’s correlation coefficient, r. When data violate the assumptions of parametric tests we can use Spearman’s rs , or for small data sets Kendall’s τ may be more accurate. We also saw that correlations can be calculated between two variables when one of those variables is a dichotomy (i.e., composed of two categories); when the categories have no underlying continuum then we use the point-biserial correlation, rpb, but when the categories do have an underlying continuum we use the biserial correlation, rb . Finally, we looked at the difference between partial correlations, in which the relationship between two variables is measured controlling for the effect that one or more variables has on both of those variables, and semi-partial correlations, in which the relationship between two variables is measured controlling for the effect that one or more variables has on only one of those variables. We also discovered that I had a guitar and, like my favourite record of the time, I was ready to ‘Take on the World’. Well, Wales at any rate …"
Cognitive Neuroscience,Neuroscience: Exploring the Brain,Neurons and Glia,Bear M.,"Learning the structural characteristics of the neuron provides insight into how neurons and their different parts work because structure correlates with function. For example, the absence of ribosomes in the axon correctly predicts that proteins in the axon terminal are provided from the soma via axoplasmic transport. A large number of mitochondria in the axon terminal correctly predicts a high energy demand. The elaborate structure of the dendritic tree appears ideally suited for receiving incoming information, and indeed, this is where most of the synapses are formed with the axons of other neurons. From the time of Nissl, the rough ER has been recognized as an important feature of neurons. What does this tell us about neurons? Recall that rough ER is a site of the synthesis of proteins destined to be inserted into the membrane. We will next see how the various proteins in the neuronal membrane give rise to the unique capabilities of neurons to transmit, receive, and store information."
Cognitive Neuroscience,Neuroscience: Exploring the Brain,The Action Potential,Bear M.,"Let’s return briefly to the example in Chapter 3 of stepping on a thumbtack. The breaking of the skin caused by the tack stretches the sensory nerve endings of the foot. Special ion channels that are sensitive to the stretching of the membrane open and allow positively charged sodium ions to enter the ends of the axons in the skin. This influx of positive charge depolarizes the membrane to threshold, and the action potential is generated. The positive charge that enters during the rising phase of the action potential spreads along the axon and depolarizes the membrane ahead to threshold. In this way, the action potential is continuously regenerated as it sweeps like a wave along the sensory axon. We now come to the step where this information is distributed to and integrated by other neurons in the central nervous system. This transfer of information from one neuron to another is called synaptic transmission, the subject of the next two chapters. It should come as no surprise that synaptic transmission, like the action potential, depends on specialized proteins in the neuronal membrane. Thus, a picture begins to emerge of the brain as a complicated mesh of interacting neuronal membranes. Consider that a typical neuron with all its neurites has a membrane surface area of about 250,000 μm². The surface area of the 85 billion neurons that make up the human brain comes to 21,250 m²—roughly the size of three soccer fields. This expanse of membrane, with its myriad specialized protein molecules, constitutes the fabric of our minds."
Cognitive Neuroscience,Neuroscience: Exploring the Brain,Synaptic Transmission,Bear M.,"This chapter has covered the basic principles of chemical synaptic transmission. The action potential that arose in the sensory nerve when you stepped on that thumbtack in Chapter 3, and that swept along the axon in Chapter 4, has now reached the axon terminal in the spinal cord. The depolarization of the terminal triggered the presynaptic entry of Ca²⁺ through voltage-gated calcium channels, which then stimulated exocytosis of the contents of synaptic vesicles. Liberated neurotransmitter diffused across the synaptic cleft and attached to specific receptors in the postsynaptic membrane. The transmitter (probably glutamate) caused transmitter-gated channels to open, which allowed positive charge to enter the postsynaptic dendrite. Because the sensory nerve was firing action potentials at a high rate, and because many synapses were activated at the same time, the EPSPs summed to bring the spike-initiation zone of the postsynaptic neuron to threshold, and this cell then generated action potentials. If the postsynaptic cell is a motor neuron, this activity will cause the release of ACh at the neuromuscular junction and muscle contraction to jerk your foot away from the tack. If the postsynaptic cell is an interneuron that uses GABA as a neurotransmitter, the activity of the cell will result in inhibition of its synaptic targets. If this cell uses a modulatory transmitter such as NE, the activity could cause lasting changes in the excitability or metabolism of its synaptic targets. It is this rich diversity of chemical synaptic interactions that allows complex behaviors (such as shrieking with pain as you jerk up your foot) to emerge from simple stimuli (such as stepping on a thumbtack). Although we surveyed chemical synaptic transmission in this chapter, we did not cover the chemistry of synaptic transmission in any detail. In Chapter 6, we’ll take a closer look at the chemical “nuts and bolts” of different neurotransmitter systems. In Chapter 15, after we’ve examined the sensory and motor systems in Part II, we’ll explore the contributions of several different neurotransmitters to nervous system function and behavior. You’ll see that the chemistry of synaptic transmission warrants all this attention because defective neurotransmission is the basis for many neurological and psychiatric disorders. And virtually all psychoactive drugs, both therapeutic and illicit, exert their effects at chemical synapses. In addition to explaining aspects of neural information processing and the effects of drugs, chemical synaptic transmission is also the key to understanding the neural basis of learning and memory. Memories of past experiences are established by modification of the effectiveness of chemical synapses in the brain. This chapter suggests possible sites of modification, ranging from changes in presynaptic Ca²⁺ entry and neurotransmitter release to alterations in postsynaptic receptors or excitability. As we shall see in Chapter 25, all of these changes are likely to contribute to the storage of information by the nervous system."
Cognitive Neuroscience,Neuroscience: Exploring the Brain,The Structure of the Nervous System,Bear M.,"Although we have covered a lot of new ground in this chapter, we have only scratched the surface of neuroanatomy. Clearly, the brain deserves its status as the most complex piece of matter in the universe. What we have presented here is a shell, or scaffold, of the nervous system and some of its contents. Understanding neuroanatomy is necessary for understanding how the brain works. This statement is just as true for an undergraduate first-time neuroscience student as it is for a neurologist or a neurosurgeon. In fact, neuroanatomy has taken on a new relevance with the advent of methods of imaging the living brain (Figure 7.30). An Illustrated Guide to Human Neuroanatomy appears as an appendix to this chapter. Use the guide as an atlas to locate various structures of interest. Labeling exercises are also provided to help you learn the names of the parts of the nervous system you will encounter in this book. In Part II, Sensory and Motor Systems, the anatomy presented in this chapter and its appendix will come alive, as we explore how the brain goes about the tasks of smelling, seeing, hearing, sensing touch, and moving."
Cognitive Neuroscience,Neuroscience: Exploring the Brain,Brain rhythms and Sleep,Bear M.,"Rhythms are ubiquitous in the vertebrate central nervous system. They also span a broad range of frequencies, from more than 500 Hz in the cortical EEG to once per year (0.00000003 Hz) for many seasonal behaviors, such as the autumn mating of deer, the winter hibernation of chipmunks, and the instinct that drives migrating swallows to return to Capistrano, California every March 19. According to local legend, in 200 years, these swallows have missed the date only twice. In some cases, these rhythms are based on intrinsic brain mechanisms; in some, they result from environmental factors; and in others, such as the SCN clock, they represent an interaction of a neural process and zeitgebers. While the purpose of some rhythms is obvious, the functions of many neural rhythms are unknown. Indeed, some rhythms may have no function at all but arise as a secondary consequence of neural interconnections that are essential for other, nonrhythmic, purposes. Among the most conspicuous yet inexplicable of brain rhythms is sleep. Sleep provides a fascinating set of problems for neuroscience. Unlike most studies of single ion channels, single neurons, or the systems mediating perception and movement, sleep research begins with profound ignorance about a most basic question: Why? We still don’t know why we spend one-third of our lives sleeping, most of that time languid and vegetative and the rest of it paralyzed and hallucinating. Sleep and dreams may have no vital function, but they can be studied and enjoyed nevertheless. Ignoring the functional question will not be a satisfying approach for long, however. For most neuroscientists, asking “Why?” remains the deepest and most challenging problem of all."
Cognitive Neuroscience,Neuroscience: Exploring the Brain,The Central Visual System,Bear M.,"In this chapter, we have outlined the organization of the sensory pathway from the eye to the thalamus to the cortex. We saw that vision actually involves the perception of numerous different properties of objects—including color, form, and movement—and these properties are processed in parallel by different cells of the visual system. This processing of information evidently requires a strict segregation of inputs at the thalamus, some limited convergence of information in the striate cortex, and finally a massive divergence of information as it is passed on to higher cortical areas. The distributed nature of the cortical processing of visual information is underscored when you consider that the output of a million ganglion cells can recruit the activity of well over a billion cortical neurons throughout the occipital, parietal, and temporal lobes! Somehow, this widespread cortical activity is combined to form a single, seamless perception of the visual world. Heed the lessons learned from the visual system. As we shall see in later chapters, the basic principles of organization in this system—parallel processing, topographic mappings of sensory surfaces, synaptic relays in the dorsal thalamus, cortical modules, and multiple cortical representations—are also features of the sensory systems devoted to hearing and touch."
Cognitive Neuroscience,Neuroscience: Exploring the Brain,The auditory and vestibular systems,Bear M.,"Hearing and balance begin with nearly identical sensory receptors, the hair cells, which are exquisitely sensitive to deflections of their stereocilia. These movement detectors are surrounded by three sets of inner ear structures that give them selectivity for three different kinds of mechanical energy: periodic waves of air pressure (sound), rotational forces (head turns), and linear forces (head tilt or acceleration). Except for the similarity in transduction, and the fact that the hair cells of both systems are located in the inner ear, the auditory and vestibular systems are quite different. The sound sensed by audition comes mainly from the external environment, while the vestibular system senses only the movements of itself. Auditory and vestibular pathways are entirely separate except perhaps at the highest levels of the cortex. Auditory information is often at the forefront of our consciousness, while vestibular sensation usually operates unnoticed to coordinate and calibrate our every movement. We have followed the auditory pathways from the ear to cerebral cortex and seen the ways in which information about sound is transformed. Variations in the density of air are converted to movements of the mechanical components of the middle and inner ear, which are transduced into neural responses. The structures of the ear and cochlea are highly specialized for the transduction of sound. However, this fact should not blind us to the considerable similarities between the organization of the auditory system and that of other sensory systems. Many analogies can be made between the auditory and visual systems. In the sensory receptors of both systems, a spatial code is established. In the visual system, the code in the photoreceptors is retinotopic; the activity of a given photoreceptor indicates light at a particular location. The receptors in the auditory system establish a spatial code that is tonotopic because of the unique properties of the cochlea. In each system, the retinotopy or tonotopy is preserved as signals are processed in secondary neurons, the thalamus, and finally in sensory cortex. The convergence of inputs from lower levels produces neurons at higher levels that have more complex response properties. Combinations of LGN inputs give rise to simple and complex receptive fields in visual cortex; similarly in the auditory system, the integration of inputs tuned to different sound frequencies yields higher level neurons that respond to complex combinations of frequencies. Another example of increasing visual complexity is the convergence of inputs from the two eyes, which yields binocular neurons that are important for depth perception. Analogously, in the auditory system, input from the two ears is combined to create binaural neurons, which are used for horizontal sound localization. These are just a few of the many similarities in the two systems. Principles governing one system can often help us understand other systems. Keep this in mind while reading about the somatic sensory system in the next chapter, and you’ll be able to predict some features of cortical organization based on the types of sensory receptors."
Cognitive Neuroscience,Neuroscience: Exploring the Brain,Spinal control of movement,Bear M.,"We can draw several conclusions from the preceding discussion of the spinal control of movement. First, a great deal has been learned about movement and its spinal control by working at different levels of analysis, ranging from biochemistry and genetics to biophysics and behavior. Indeed, a complete understanding, whether of excitation–contraction coupling or central pattern generation, requires knowledge derived from every approach. Second, sensation and movement are inextricably linked even at the lowest levels of the neural motor system. The normal function of the alpha motor neuron depends on direct feedback from the muscles themselves and indirect information from the tendons, joints, and skin. Third, the spinal cord contains an intricate network of circuits for the control of movement; it is far more than just a conduit for somatic sensory and motor information. Evidently, coordinated and complex patterns of activity in these spinal circuits can be driven by relatively crude descending signals. This leaves the question of precisely what the upper motor neurons contribute to motor control—the subject of the next chapter."
Cognitive Neuroscience,Neuroscience: Exploring the Brain,Brain control of movement,Bear M.,"Let’s return to the example of the baseball pitcher one last time to put the different pieces of the motor control puzzle together. Imagine the pitcher walking to the mound. The spinal circuits of the crossed extensor reflex are engaged and coordinated by descending commands on the ventromedial pathways. Extensors contract, flexors relax; flexors contract, extensors relax. Once on the mound, the pitcher is joined by the umpire. Into his outstretched hand the umpire drops a new baseball. The added weight stretches the flexors of the arm. Group Ia axons become more active and cause monosynaptic excitation of the motor neurons innervating the flexors. The muscles contract to hold the ball up against gravity. He is now ready to pitch. His neocortex is fully engaged and active as he looks at the catcher for the hand signal that tells him the type of pitch to throw. At the same time, the ventromedial pathways are working to maintain his standing posture. Although his body is still, the neurons of the ventral horns of the spinal cord are firing steadily under the influence of the ventromedial pathways, keeping the extensors of the lower leg activated. The catcher flashes the sign for the curve ball. The sensory information is communicated from the occipital cortex to the parietal and prefrontal cortex. These regions of cortex along with area 6 begin planning the movement strategy. The batter steps up to the plate and is ready. Activity cycling through the basal ganglia increases, triggering the initiation of the pitch. In response to this input to the cerebral cortex, SMA activity increases, followed immediately by the activation of M1. Now instructions are sweeping down the axons of the lateral pathways. The cerebellum, activated by the corticopontocerebellar inputs, uses these instructions to coordinate the timing and force of the descending activity so the proper sequence of muscle contractions can occur. Cortical input to the reticular formation leads to the release of the antigravity muscles from reflex control. Finally, lateral pathway signals engage the motor neurons and interneurons of the spinal cord, which cause muscles of the arms and legs to contract. The pitcher winds up and throws. The batter swings. The ball sails over the left-field fence. The crowd jeers; the manager curses; the team owner frowns. Even as the pitcher’s cerebellum goes to work making adjustments for the next pitch, his body reacts. His face flushes; he sweats; he’s angry and anxious. But these latter reactions are not the stuff of the somatic motor system. These are topics of Part III, The Brain and Behavior."
Cognitive Neuroscience,Neuroscience: Exploring the Brain,Brain mechanisms of emotion,Bear M.,"We all know what emotions are—those feelings we have that we call happiness, sadness, and so on. But what exactly are those feelings? As evidenced by the diverse theories we have outlined, there is a great deal of uncertainty. More than a hundred years after the James–Lange theory was proposed, there remains controversy about the extent to which emotions cause changes in the body or bodily changes cause emotions. We do know from brain imaging studies that emotions are associated with widespread brain activation. Some of the structures involved are part of the limbic system, and other structures are not. But even with images of brain activity in various emotional states, understanding the neural basis of emotional experience is challenging. We don’t know which of the active areas are responsible for the feelings. Is it the most active area, all of the areas, or something else? What should we make of the observation that some brain structures are activated in multiple emotional states while others are more specific to particular emotions? For that matter, is it even correct to think of brain activity as reflecting feelings, or might feelings be emergent sensations based on combinations of active neurons, none of which independently signals an emotion? In this chapter, we have focused on a handful of brain structures for which there is particularly strong evidence for involvement in emotion. A way to look at our current state of understanding is that the combined lesion, stimulation, and brain imaging studies have done a good job identifying structures that are candidates for emotional processing. It will take a good deal more work to figure out what various cortical and subcortical areas contribute. Emotional experiences are the result of complex interactions among sensory stimuli, brain circuitry, past experiences, and the activity of neurotransmitter systems. In light of this complexity, we probably should not be surprised that humans exhibit a broad spectrum of emotional and mood disorders, as we will see in Chapter 22. When thinking about the neural basis of emotion, keep in mind that the structures apparently involved in emotion also have other functions. For a considerable time after Broca defined the limbic lobe, it was thought to be primarily an olfactory system. And even though our perspective has changed much since Broca’s time, parts of the brain involved in olfaction have been included in the definition of the limbic system. We will see in Chapter 24 that some of the limbic structures are also important for learning and memory. Emotions are nebulous experiences that influence our brains and behavior in many ways, so it seems logical that emotional processing should be intertwined with other brain functions."
Cognitive Neuroscience,Neuroscience: Exploring the Brain,Language,Bear M.,"Language was one of the most important steps in human evolution. Communication between people is such a fundamental aspect of being human that it is difficult to imagine life without language. Current estimates are that the human capacity for language evolved relatively recently, around 100,000 years ago. While animals use a great diversity of sounds and behaviors to communicate, none of these come close to the elaborate and flexible system of language and speech used by humans. Aspects of language acquisition and use have been fruitfully studied in songbirds and nonhuman primates, but unlike other brain systems, the study of human language requires experiments and observations in humans. For this reason, experimental approaches are largely limited to behavioral studies of language acquisition and function, the consequences of lesions, effects of brain stimulation, and brain imaging with fMRI and PET. Even with these few techniques, however, a great deal has been learned. Consistent with the locations of sensory and motor areas in the brain, the basics of language organization can be understood. Early studies focused attention on Broca’s area, which is near the motor cortex and associated with speech production, and Wernicke’s area, which is near the auditory cortex and associated with speech comprehension. These observations are still of clinical use today. More recent research has shown that language processing is far more complex and engages far more of the brain than implied by the Wernicke–Geschwind model. Brain imaging and stimulation studies have revealed widespread brain areas in both hemispheres that are involved in language and that vary from one person to the next. From our perspective today, the complexity of language and its extensive representation in the brain are not so surprising because language involves many different components, such as understanding the basis of words in sound, the meanings of words, the grammar used to combine words into meaningful statements, naming objects, producing speech, and on and on. As in research on other brain systems for sensation, motor output, emotion, and so on, we are interested in the extent to which language processing involves a collection of interacting subsystems for different language skills. There is clearly still much to be learned. Further brain imaging studies will hopefully clarify the organization of language systems in the brain at a finer scale than was possible from studying the consequences of brain lesions and perhaps identify distinct circuits that serve different functions."
Cognitive Neuroscience,Neuroscience: Exploring the Brain,Memory Systems,Bear M.,"Far from being a computer with fixed connections, the human brain is constantly changing as a result of experience. We use working memory to temporarily hold onto information, and the patterns of sensory input from some of our experiences are assembled into permanent engrams. As a child you learned to do a somersault, and the sequence of movements was unconsciously stored for use time after dizzying time. You learned the structure of the brain and are able to impress Aunt Tilly by making a sketch showing the location of the medulla oblongata. We cannot identify the precise neurons and synapses involved in storing nondeclarative or declarative memories, but research is moving us closer to such an understanding. We know that learning and memory involve changes widely across the brain. Structures in the medial temporal lobe and diencephalon are critical for memory consolidation, and engrams are stored in the neocortex through interactions with the hippocampus and other structures. Specifying precisely what each brain structure contributes to learning and memory continues to challenge researchers. We have seen that memories can be classified based on duration, the kind of information stored, and the brain structures involved. Early brain research relied on interpreting the effects of brain lesions on amnesia. From the case of H.M. alone, a tremendous amount was learned about memory in the human brain. The distinct types of memory, and the fact that one type can be disrupted without affecting others in amnesia, indicate that multiple brain systems are used for memory storage. More recent research uses human brain imaging and molecular genetic techniques to examine memory formation and sort out the temporal processes and multiple systems. There is even hope that one day there will be a treatment to significantly reduce the deleterious consequences of traumatic memories. In this chapter we have focused on questions about where memories are stored and how different brain structures interact. But what is the physiological basis for the memory storage? When we try to remember a phone number, an interruption can make us forget, suggesting that memories are initially held in a particularly fragile form. Long-term memory is much more robust; it can survive interruption, anesthesia, and the normal bumps and traumas of life. Because of this robustness, it is thought that memories are ultimately stored in structural brain changes. The nature of these structural changes in the brain is the subject of Chapter 25."
Cognitive Neuroscience,Neuroscience: Exploring the Brain,Molecular mechanisms of learning and memory,Bear M.,"Learning and memory can occur at synapses. Regardless of the species, brain location, and memory type, many of the underlying mechanisms appear to be universal. Events are represented first as changes in the electrical activity of the brain, then as intracellular second messengers, and next as modifications of existing synaptic proteins. These temporary changes are converted to permanent ones, and long-term memories, by altering the structure of the synapse. In many forms of memory, this involves new protein synthesis and the assembly of new microcircuits. In other forms of memory, existing circuits may be disassembled. In either case, learning requires many of the same mechanisms that were used to refine brain circuitry during development. One universal feature is the involvement of Ca²⁺. Clearly, calcium does far more than build strong bones and teeth. Not only is it critical for neurotransmitter secretion and muscle contraction, but it is also involved in nearly every form of synaptic plasticity. Because it is a charge-carrying ion on the one hand and a potent second messenger substance on the other, Ca²⁺ has a unique ability to directly couple electrical activity with long-term changes in the brain. Can basic neuroscience research take us from ions to intelligence? From calcium to cognition? If you remember what you have learned in this textbook, and if synaptic plasticity truly is the basis of declarative memory, it would appear that the answer is yes."
Methods,Statistical Rethinking,The golem of prague,McElreath R.,"This first chapter has argued for a rethinking of popular statistical and scientific philosophy. Instead of choosing among various black-box tools for testing null hypotheses, we should learn to build and analyze multiple non-null models of natural phenomena. To support this goal, the chapter introduced Bayesian inference, model comparison, multilevel models, and graphical causal models. The remainder of the book is organized into four interdependent parts. Foundational Concepts (Chapters 2 and 3): These chapters introduce Bayesian inference and the basic tools for performing Bayesian calculations. They move quite slowly and emphasize a purely logical interpretation of probability theory. Bayesian Multiple Linear Regression (Chapters 4 through 9): This section builds multiple linear regression as a Bayesian tool, supporting causal inference by analyzing separate causal models to determine which variables to include. These chapters emphasize plotting results instead of attempting to interpret estimates of individual parameters. Problems of model complexity—overfitting—also feature prominently, introducing information theory and formal model comparison in Chapter 7. Generalized Linear Models (Chapters 9 through 12): This part presents generalized linear models of several types. Chapter 9 introduces Markov chain Monte Carlo (MCMC) methods used to fit the models in later chapters. Chapter 10 introduces maximum entropy as an explicit procedure to help design and interpret these models. Chapters 11 and 12 detail the models themselves. Advanced Models and Applications (Chapters 13 through 16): This section covers multilevel models, as well as specialized models addressing measurement error, missing data, and spatial correlation. This material is fairly advanced but proceeds in the same mechanistic way as earlier material. Chapter 16 departs from the rest of the book by deploying theoretical models expressed directly as statistical models, rather than generalized linear types. The final chapter, Chapter 17, returns to some of the issues raised in this first one. At the end of each chapter, there are practice problems ranging from easy to hard. These problems help you test your comprehension, with harder ones expanding on the material and introducing new examples and obstacles."
Methods,Statistical Rethinking,Small worlds and large worlds,McElreath R.,"This chapter introduced the conceptual mechanics of Bayesian data analysis. The target of inference in Bayesian inference is a posterior probability distribution. Posterior probabilities state the relative numbers of ways each conjectured cause of the data could have produced the data. These relative numbers indicate plausibilities of the different conjectures. These plausibilities are updated in light of observations, a process known as Bayesian updating. More mechanically, a Bayesian model is a composite of variables and distributional definitions for these variables. The probability of the data, often called the likelihood, provides the plausibility of an observation (data), given a fixed value for the parameters. The prior provides the plausibility of each possible value of the parameters, before accounting for the data. The rules of probability tell us that the logical way to compute the plausibilities, after accounting for the data, is to use Bayes’ theorem. This results in the posterior distribution. In practice, Bayesian models are fit to data using numerical techniques, like grid approximation, quadratic approximation, and Markov chain Monte Carlo (MCMC). Each method imposes different trade-offs."
Methods,Statistical Rethinking,Sampling the imaginary,McElreath R.,"This chapter introduced the basic procedures for manipulating posterior distributions. Our fundamental tool is samples of parameter values drawn from the posterior distribution. Working with samples transforms a problem of integral calculus into a problem of data summary. These samples can be used to produce intervals, point estimates, posterior predictive checks, as well as other kinds of simulations. Posterior predictive checks combine uncertainty about parameters, as described by the posterior distribution, with uncertainty about outcomes, as described by the assumed likelihood function. These checks are useful for verifying that your software worked correctly. They are also useful for prospecting for ways in which your models are inadequate. Once models become more complex, posterior predictive simulations will be used for a broader range of applications. Even understanding a model often requires simulating implied observations. We’ll keep working with samples from the posterior to make these tasks as easy and customizable as possible."
Methods,Statistical Rethinking,Geocentric models,McElreath R.,"This chapter introduced the simple linear regression model, a framework for estimating the association between a predictor variable and an outcome variable. The Gaussian distribution comprises the likelihood in such models, because it counts up the relative numbers of ways different combinations of means and standard deviations can produce an observation. To fit these models to data, the chapter introduced quadratic approximation of the posterior distribution and the tool quap. It also introduced new procedures for visualizing posterior distributions and posterior predictions. The next chapter expands on these concepts by introducing regression models with more than one predictor variable. The basic techniques from this chapter are the foundation of most of the examples in later chapters. So if much of the material was new to you, it might be worth reviewing the chapter now, before pressing onwards."
Methods,Statistical Rethinking,The many variables & the spurious waffles,McElreath R.,"This chapter introduced multiple regression, a way of constructing descriptive models for how the mean of a measurement is associated with more than one predictor variable. The defining question of multiple regression is: What is the value of knowing each predictor, once we already know the other predictors? Implicit in this question are: (1) a focus on the value of the predictors for description of the sample, instead of forecasting a future sample; and (2) the assumption that the value of each predictor does not depend upon the values of the other predictors. In later chapters, we confront these two issues. But before that, in the next chapter, we’ll see how adding predictor variables can create as many problems as it can solve."
Methods,Statistical Rethinking,The haunted DAG & the causal terror,McElreath R.,"Multiple regression is no oracle. It is logical, but the relationships it describes are conditional associations, not causal influences. Therefore, additional information from outside the model is needed to make sense of it. This chapter presented introductory examples of some common frustrations: multicollinearity, post-treatment bias, and collider bias. Solutions to these frustrations can be organized under a coherent framework in which hypothetical causal relations among variables are analyzed to locate and cope with confounding. In all cases, causal models exist outside the statistical model and can be difficult to test. However, it is possible to reach valid causal inferences in the absence of experiments. This is good news because we often cannot perform experiments, both for practical and ethical reasons."
Methods,Statistical Rethinking,Ulysses' Compass,McElreath R.,"This chapter has been a marathon. It began with the problem of overfitting, a universal phenomenon by which models with more parameters fit a sample better, even when the additional parameters are meaningless. Two common tools were introduced to address overfitting: regularizing priors and estimates of out-of-sample accuracy (WAIC and PSIS). Regularizing priors reduce overfitting during estimation, and WAIC and PSIS help estimate the degree of overfitting. Practical functions in the rethinking package were introduced to help analyze collections of models fit to the same data. If you are after causal estimates, then these tools will mislead you. So models must be designed through some other method, not selected on the basis of out-of-sample predictive accuracy. But any causal estimate will still overfit the sample. So you always have to worry about overfitting, measuring it with WAIC/PSIS and reducing it with regularization."
Methods,Statistical Rethinking,Conditional Manatees,McElreath R.,"This chapter introduced interactions, which allow for the association between a predictor and an outcome to depend upon the value of another predictor. While you can’t see them in a Directed Acyclic Graph (DAG), interactions can be important for making accurate inferences. Interactions can be difficult to interpret, so the chapter also introduced triptych plots that help in visualizing the effect of an interaction. No new coding skills were introduced, but the statistical models considered were among the most complicated so far in the book. To go any further, we’re going to need a more capable conditioning engine to fit our models to data. That’s the topic of the next chapter."
Methods,Statistical Rethinking,Markov Chain,McElreath R.,"This chapter has been an informal introduction to Markov chain Monte Carlo (MCMC) estimation. The goal has been to introduce the purpose and approach of MCMC algorithms. The major algorithms introduced were the Metropolis, Gibbs sampling, and Hamiltonian Monte Carlo algorithms. Each has its advantages and disadvantages. The ulam function in the rethinking package was introduced. It uses the Stan (mc-stan.org) Hamiltonian Monte Carlo engine to fit models as they are defined in this book. General advice about diagnosing poor MCMC fits was introduced by the use of a couple of pathological examples. In the next chapters, we use this new power to learn new kinds of models."
Methods,Statistical Rethinking,Big entropy and the generalized linear model,McElreath R.,"This chapter has been a conceptual, not practical, introduction to maximum entropy and generalized linear models. The principle of maximum entropy provides an empirically successful way to choose likelihood functions. Information entropy is essentially a measure of the number of ways a distribution can arise, according to stated assumptions. By choosing the distribution with the biggest information entropy, we thereby choose a distribution that obeys the constraints on outcome variables, without importing additional assumptions. Generalized linear models arise naturally from this approach, as extensions of the linear models in previous chapters. The necessity of choosing a link function to bind the linear model to the generalized outcome introduces new complexities in model specification, estimation, and interpretation. You’ll become comfortable with these complexities through examples in later chapters."
Methods,Statistical Rethinking,God spiked the integers,McElreath R.,"This chapter described some of the most common generalized linear models, particularly those used to model counts. It is important to never convert counts to proportions before analysis, because doing so destroys information about sample size. A fundamental difficulty with these models is that parameters are on a different scale, typically log-odds (for binomial) or log-rate (for Poisson), than the outcome variable they describe. Therefore, computing implied predictions is even more important than before."
Methods,Statistical Rethinking,Monsters and mixtures,McElreath R.,"This chapter introduced several new types of regression, all of which are generalizations of generalized linear models (GLMs). Ordered logistic models are useful for categorical outcomes with a strict ordering. They are built by attaching a cumulative link function to a categorical outcome distribution. Zero-inflated models mix together two different outcome distributions, allowing us to model outcomes with an excess of zeros. Models for overdispersion, such as beta-binomial and gamma-Poisson, draw the expected value of each observation from a distribution that changes shape as a function of a linear model. The next chapter further generalizes these model types by introducing multilevel models."
Methods,Statistical Rethinking,Models with memory,McElreath R.,"This chapter has been an introduction to the motivation, implementation, and interpretation of basic multilevel models. It focused on varying intercepts, which achieve better estimates of baseline differences among clusters in the data. They achieve better estimates because they simultaneously model the population of clusters and use inferences about the population to pool information among parameters. From another perspective, varying intercepts are adaptively regularized parameters, relying upon a prior that is itself learned from the data. All of this is a foundation for the next chapter, which extends these concepts to additional types of parameters and models."