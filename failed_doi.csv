doi,title,author,abstract
10.2307/2786545,An Experimental Study of the Small World Problem,Travers J.,"Arbitrarily selected individuals (N=296) in Nebraska and Boston are asked to generate acquaintance chains to a target person in Massachusetts, employing ""the small world method"" (Milgram, 1967). Sixty-four chains reach the target person. Within this group the mean number of intermediaries between starters and targets is 5.2. Boston starting chains reach the target person with fewer intermediaries than those starting in Nebraska; subpopulations in the Nebraska group do not differ among themselves. The funneling of chains through sociometric ""stars"" is noted, with 48 per cent of the chains passing through three persons before reaching the target. Applications of the method to studies of large scale social structure are discussed."
10.1086/jar.33.4.3629752,An Information Flow Model for Conflict and Fission in Small Groups,Zachary W.,"Data from a voluntary association are used to construct a new formal model for a traditional anthropological problem, fission in small groups. The process leading to fission is viewed as an unequal flow of sentiments and information across the ties in a social network. This flow is unequal because it is uniquely constrained by the contextual range and sensitivity of each relationship in the network. The subsequent differential sharing of sentiments leads to the formation of subgroups with more internal stability than the group as a whole, and results in fission. The Ford-Fulkerson labeling algorithm allows an accurate prediction of membership in the subgroups and of the locus of the fission to be made from measurements of the potential for information flow across each edge in the network. Methods for measurement of potential information flow are discussed, and it is shown that all appropriate techniques will generate the same predictions."
10.48550/ARXIV.2210.03216,Beyond the shortest path: the path length index as a distribution,Santos L.,"The traditional complex network approach considers only the shortest paths from one node to another, not taking into account several other possible paths. This limitation is significant, for example, in urban mobility studies. In this short report, as the first steps, we present an exhaustive approach to address that problem and show we can go beyond the shortest path, but we do not need to go so far: we present an interactive procedure and an early stop possibility. After presenting some fundamental concepts in graph theory, we presented an analytical solution for the problem of counting the number of possible paths between two nodes in complete graphs, and a depth-limited approach to get all possible paths between each pair of nodes in a general graph (an NP-hard problem). We do not collapse the distribution of path lengths between a pair of nodes into a scalar number, we look at the distribution itself - taking all paths up to a pre-defined path length (considering a truncated distribution), and show the impact of that approach on the most straightforward distance-based graph index: the walk/path length."
10.48550/ARXIV.2110.01866,Social physics,Jusup M.,"Recent decades have seen a rise in the use of physics methods to study different societal phenomena. This development has been due to physicists venturing outside of their traditional domains of interest, but also due to scientists from other disciplines taking from physics the methods that have proven so successful throughout the 19th and the 20th century. Here we dub this field 'social physics' and pay our respect to intellectual mavericks who nurtured it to maturity. We do so by reviewing the current state of the art. Starting with a set of topics that are at the heart of modern human societies, we review research dedicated to urban development and traffic, the functioning of financial markets, cooperation as the basis for our evolutionary success, the structure of social networks, and the integration of intelligent machines into these networks. We then shift our attention to a set of topics that explore potential threats to society. These include criminal behaviour, large-scale migrations, epidemics, environmental challenges, and climate change. We end the coverage of each topic with promising directions for future research. Based on this, we conclude that the future for social physics is bright. Physicists studying societal phenomena are no longer a curiosity, but rather a force to be reckoned with. Notwithstanding, it remains of the utmost importance that we continue to foster constructive dialogue and mutual respect at the interfaces of different scientific disciplines."
10.31234/osf.io/n3t6j,The effect of diversity and social interaction on cognitive search: An agent-based simulation,Rocca R.,"Cognitive search, commonly conceptualized as information foraging through mental spaces, is the foundation of many daily tasks. Cognitive search in real-world contexts is often performed jointly. Yet, the way social interaction impacts cognitive search mechanisms and outcomes is understudied. Over three agent-based simulations, we investigated how social interaction modulates performance in cognitive search, focusing on two fundamental parameters: agents’ cognitive diversity and properties of their interaction. In Experiment 1, agents took turns in naming animals following a strict turn-taking protocol. We observed that moderate levels of diversity were beneficial to performance and induced more exploratory search behavior, but performance dropped dramatically at the highest levels of diversity. In Experiment 2, these detrimental effects of high diversity were mitigated by allowing agents to follow other and more flexible interaction protocols, which induced different and more efficient search strategies. In Experiment 3, we observed that endowing agents with more flexible cognitive systems (working memory) also had a positive impact on their performance. With this work, we provide new insights into how cognitive diversity and social interaction shape performance in joint cognitive search, and we present an innovative, flexible, and controllable computational paradigm to study emergent and mechanistic aspects of joint cognitive search."
10.1101/162313,The Evolution of Fair Offers with Low Rejection Thresholds in the Ultimatum Game,Schrank J.,"The ultimatum game (UG) is widely used in economic and anthropological research to investigate fairness by how one player proposes to divide a resource with a second player who can reject the offer. In these contexts, fairness is understood as offers that are more generous than predicted by the subgame perfect Nash equilibrium (SPNE). A surprising and robust result of UG experiments is that proposers offer much more than the SPNE. These results have spawned many models aimed at explaining why players do not conform to the SPNE by showing how Nash equilibrium strategies can evolve far from the SPNE. However, empirical data from UG experiments indicate that players do not use Nash equilibrium strategies, but rather make generous offers while rejecting only very low offers. To better understand why people behave this way, we developed an agent-based model to investigate how generous strategies could evolve in the UG. Using agents with generic biological properties, we found that fair offers can readily evolve in structured populations even while rejection thresholds remain relatively low. We explain the evolution of fairness as a problem of the efficient conversion of resources into the production of offspring at the level of the group. Significance Statement Human generosity is widespread and far exceeds that of other social animals. Generosity is often studied experimentally with the ultimatum game, in which a proposer offers a split and a responder can either accept it or cancel the whole deal. A surprising result of ultimatum game experiments is that players are much more generous than predicted while only rejecting very low offers. This has presented a theoretical puzzle, since mathematical models have generally relied on high rejection levels—just below offer levels—to maintain generosity. Using evolutionary simulations, we explain both generous offers and the rejection of only low offers as a solution to the problem of how groups can efficiently convert resources into the production of offspring."
10.1086/225469,The Strength of Weak Ties,Granovetter M.,"Analysis of social networks is suggested as a tool for linking micro and macro levels of sociological theory. The procedure is illustrated by elaboration of the macro implications of one aspect of small-scale interaction: the strength of dyadic ties. It is argued that the degree of overlap of two individuals' friendship networks varies directly with the strength of their tie to one another. The impact of this principle on diffusion of influence and information, mobility opportunity, and community organization is explored. Stress is laid on the cohesive power of weak ties. Most network models deal, implicitly, with strong ties, thus confining their applicability to small, well-defined groups. Emphasis on weak ties lends itself to discussion of relations between groups and to analysis of segments of social structure not easily defined in terms of primary groups."
10.1016/j.bandc.2005.11.00,Understanding the effects of moving visual stimuli on unilateral neglect following stroke,Plummer P.,"Moving visual stimuli have been shown to reduce unilateral neglect (ULN), however, the mechanisms underlying these effects remain poorly understood. This study compared lateralised and non-lateralised moving visual stimuli to investigate whether the spatial characteristics or general alerting properties of moving visual stimuli are responsible for reducing neglect. Post-stroke left neglect patients as well as healthy and patient control subjects were tested on a computerised line bisection task under six visual stimulus conditions. The key finding was that, relative to the no stimulus condition, leftward moving and left-sided moving visual stimuli shifted neglect patients’ bisection errors leftward while the non-lateralised random moving visual stimuli did not reduce neglect patients’ rightward bisection errors. The results provide evidence that spatial characteristics rather than general alerting properties of moving visual stimuli reduce rightward bisection errors in ULN. Moreover, the pattern of findings strongly supports the notion that moving visual stimuli reduce neglect by capturing attention and drawing it to a spatial location rather than by activating the attentional system via superior collicular neurons."
10.1016/j.neuroimage.2011.06.07,Neural correlates of training-related working-memory gains in old age,Brehmer Y.,"Working memory (WM) functioning declines in old age. Due to its impact on many higher-order cognitive functions, investigating whether training can modify WM performance has recently been of great interest. We examined the relationship between behavioral performance and neural activity following five weeks of intensive WM training in 23 healthy older adults (M = 63.7 years). 12 participants received adaptive training (i.e. individually adjusted task difficulty to bring individuals to their performance maximum), whereas the others served as active controls (i.e. fixed low-level practice). Brain activity was measured before and after training, using fMRI, while subjects performed a WM task under two difficulty conditions. Although there were no training-related changes in WM during scanning, neocortical brain activity decreased post training and these decreases were larger in the adaptive training group than in the controls under high WM load. This pattern suggests intervention-related increases in neural efficiency. Further, there were disproportionate gains in the adaptive training group in trained as well as in non-trained (i.e. attention, episodic memory) tasks assessed outside the scanner, indicating the efficacy of the training regimen. Critically, the degree of training-related changes in brain activity (i.e. neocortical decreases and subcortical increases) was related to the maximum gain score achieved during the intervention period. This relationship suggests that the decreased activity, but also specific activity increases, observed were functionally relevant."
10.4324/9781315672236,Foundations of Sensation and Perception,Mather G.,"Perception involves highly complex neural processes that consume a substantial proportion of the brain’s cerebral cortex. Sensation, Perception, and Sensory Modality Sensations (also known as qualia) are primitive mental states or experiences induced by sensory stimulation. Perceptions are complex, organized, and meaningful experiences of objects or events. Qualia can be divided into seven distinct sensory modalities: audition, gustation, nociception, olfaction, somatosensation, the vestibular sense, and vision. The modalities differ in terms of the physical stimuli that excite them, the neural structures involved in transduction and sensory analysis, and the functions they serve. In humans, a much greater area of cortex is devoted to vision than to the other senses. The three key elements of perception are: Stimuli Neural responses Perceptions Stimuli generate neural responses which in turn lead to perceptual experiences. Psychophysics studies the relation between stimuli and perceptual experience, while neuroscience studies the relation between stimuli and neural responses. Psychophysical linking hypotheses propose specific links between perception and neural responses, as part of theories in computational neuroscience. Psychophysics Psychophysical methods to study perception were developed by Weber and Fechner in the 1800s, who established some fundamental laws governing the relation between sensory stimuli and sensation. Basic concepts in psychophysics include: Sensory thresholds Sensory magnitude Sensory adaptation Psychophysical linking hypotheses Cognitive Neuroscience Methods used to study sensation and perception include: Lesion experiments Clinical cases Single-unit recordings Neuroimaging Direct brain stimulation Basic concepts include: Neural impulses and transduction Hierarchical processing Specific nerve energy Selectivity Univariance Organization Plasticity Noise Computational Neuroscience The foundations of computational neuroscience were laid by three mathematicians: Alan Turing introduced the concept of universal computation Claude Shannon developed Information Theory David Marr introduced the three-level distinction between computational theory, representation, and hardware implementation Basic concepts include: Analog and symbolic representation Computation"
10.7551/mitpress/12441.001.0001,"Active Inference: The Free Energy Principle in Mind, Brain, and Behavior",Friston K.,"Active Inference is a theory of how living artifacts sustain their existence by minimizing surprise—or a tractable proxy to surprise, variational free energy—via perception and action. In this chapter, we have sought to motivate this idea starting from a Bayesian treatment of perception as inference and extending this to the domain of action. Bayesian inference rests on a generative model of how sensory observations are generated, which encodes (probabilistically) the organism’s implicit knowledge of the world—formalized as prior beliefs and the expected outcomes under alternative states and policies. The specific take of Active Inference forces us to revisit the usual semantics of a prior in Bayesian inference. Expected states are preferred and include the organism’s conditions for survival (e.g., niche-specific goal states), whereas their opposite—surprising states—are dis-preferred. In this way, by fulfilling their expectations, Active Inference agents ensure their own survival. Given the important links between the notion of priors and the conditions that undergird an organism’s existence, we can also say that in Active Inference, the identity of an agent is isomorphic with its priors. This terminology will become more familiar later in the book. Note that in this view, surprise (or sometimes surprisal) is a formal construct of information theory and not necessarily equivalent to a (folk) psychological construct. Roughly, the more the organism’s state differs from the prior (which encodes the preferred states), the more it is surprising—hence Active Inference amounts to the idea that an organism (or its brain) has to actively minimize its surprise to stay alive. Under certain conditions, surprise minimization can be construed as the reduction of the discrepancy between the model and the world. More generally, the quantity that is actually minimized in Active Inference is variational free energy. Variational free energy is an (upper-bound) approximation to surprise and can be minimized efficiently using chemical or neuronal message passing and information that is available to the organism’s generative model. Importantly, both perception and action minimize variational free energy in complementary ways: by refining their (posterior belief) estimate and by performing actions that selectively sample what is expected. Furthermore, Active Inference also minimizes expected free energy by following policies associated with minimal ambiguity and risk. Expected free energy then extends Active Inference to prospective and counterfactual forms of inference. This completes our journey along the low road to Active Inference. In Chapter 3, we will travel the high road, which reaches the same conclusion on the basis of first principles and self-organization."
10.31234/osf.io/e437b,Putting psychology to the test: Rethinking model evaluation through benchmarking and prediction,Rocca R.,"Consensus on standards for evaluating models and theories is an integral part of every science. Nonetheless, in psychology, relatively little focus has been placed on defining reliable communal metrics to assess model performance. Evaluation practices are often idiosyncratic, and are affected by a number of shortcomings (e.g., failure to assess models' ability to generalize to unseen data) that make it difficult to discriminate between good and bad models. Drawing inspiration from fields like machine learning and statistical genetics, we argue in favor of introducing common benchmarks as a means of overcoming the lack of reliable model evaluation criteria currently observed in psychology. We discuss a number of principles benchmarks should satisfy to achieve maximal utility; identify concrete steps the community could take to promote the development of such benchmarks; and address a number of potential pitfalls and concerns that may arise in the course of implementation. We argue that reaching consensus on common evaluation benchmarks will foster cumulative progress in psychology, and encourage researchers to place heavier emphasis on the practical utility of scientific models."
10.1093/schbul/sby016.383,WHY VALIDATION MATTERS: A DEMONSTRATION PREDICTING ANTIPSYCHOTIC RESPONSE USING 5 RCTS,Chekroud A.,"Background Machine learning methods hold promise for making more effective, personalized treatment decisions to improve outcomes and reduce the cost of care. The use of these techniques remains nascent in psychiatry, and relatively little research has focused on the extent to which models derived in one sample make accurate predictions in unseen samples. Statistical research indicates that model performance in unseen samples is generally lower than performance in the derivation sample. Methods We investigate the generalizability of machine learned models using data from five multi-site randomized controlled trials of antipsychotic efficacy (total N = 1511). We include 125 predictor variables collected at baseline in all five trials, including demographics, psychological/behavioral scales (AIMS, BARS, CGI, PANSS, and SARS), vital signs, complete blood count, blood chemistry, and urinalysis. Using elastic net regression, we predicted 4-week treatment outcomes according to a binary cut-point of 25% reduction in PANSS scores. This study compared model performance for a range of internal and external validation methodologies. Results First, we trained a separate model on each of the five trials with no internal or external validation and obtained single-trial balanced accuracies from 74.6% to 100%. When each trial was split into a 50% training set and 50% holdout set, the balanced accuracies on the holdout test set were between 48% and 60.6%. When models were trained on each trial using 10-fold cross validation, balanced accuracies ranged from 50% to 73.7%. When each model was trained on a single trial and then sequentially tested on each of the four other trials, the mean balanced accuracy for each trial ranged from 50.5% to 54.2%. Finally, when the model was trained on four trials combined and tested on the one trial left out (leave-one-trial-out validation), balanced accuracies ranged from 48.9% to 58.7%. Discussion The performance of models predicting antipsychotic treatment response is highly affected by the validation routine chosen. Performance estimated using one trial—even using internal cross-validation—is drastically higher than the performance obtained when the same model is tested on independent data from other clinical trials with similar protocols. These findings present considerable cause for concern regarding the interpretation of predictive analyses based on a single, multi-site trial."
10.1027/1864-9335/a00017,Replicating and Fixing Failed Replications: The Case of Need for Cognition and Argument Quality,Luttrell A.,"Recent large-scale replication efforts have raised the question: how are we to interpret failures to replicate? Many have responded by pointing out conceptual or methodological discrepancies between the original and replication studies as potential explanations for divergent results, as well as emphasizing the importance of contextual moderators. To illustrate the importance of accounting for discrepancies between original and replication studies, as well as moderators, we turn to a recent example of a failed replication effort. Previous research has shown that individual differences in need for cognition interact with a message’s argument quality to affect evaluation (Cacioppo, Petty, & Morris, 1983). However, a recent attempt failed to replicate this outcome (Ebersole et al., in press). We propose that the latter study’s null result was due to conducting a non-optimal replication attempt. We thus conducted a new study that manipulated the key features that we propose created non-optimal conditions in the replication effort. The current results replicated the original need for cognition × argument quality interaction but only under the “optimal” conditions (closer to the original study’s method and accounting for subsequently identified moderators). Under the non-optimal conditions, mirroring those used by Ebersole et al., results replicated the failure to replicate the target interaction. These findings emphasize the importance of informed replication, an approach to replication that pays close attention to ongoing developments identified in an effect’s broader literature."
10.1073/pnas.220886312,A discipline-wide investigation of the replicability of Psychology papers over the past two decades,Youyou W. ,"Conjecture about the weak replicability in social sciences has made scholars eager to quantify the scale and scope of replication failure for a discipline. Yet small-scale manual replication methods alone are ill-suited to deal with this big data problem. Here, we conduct a discipline-wide replication census in science. Our sample (N = 14,126 papers) covers nearly all papers published in the six top-tier Psychology journals over the past 20 y. Using a validated machine learning model that estimates a paper’s likelihood of replication, we found evidence that both supports and refutes speculations drawn from a relatively small sample of manual replications. First, we find that a single overall replication rate of Psychology poorly captures the varying degree of replicability among subfields. Second, we find that replication rates are strongly correlated with research methods in all subfields. Experiments replicate at a significantly lower rate than do non-experimental studies. Third, we find that authors’ cumulative publication number and citation impact are positively related to the likelihood of replication, while other proxies of research quality and rigor, such as an author’s university prestige and a paper’s citations, are unrelated to replicability. Finally, contrary to the ideal that media attention should cover replicable research, we find that media attention is positively related to the likelihood of replication failure. Our assessments of the scale and scope of replicability are important next steps toward broadly resolving issues of replicability."
10.1093/oxfordhb/9780199959303.013.0003,Intrinsic and Extrinsic Value,Rønnow-Rasmussen T.,"Section 2.1 identifies three notions of intrinsic value: the finality sense understands it as value for its own sake, the supervenience sense identifies it with value that depends exclusively on the bearer’s internal properties, and the nonderivative sense describes intrinsic value as value that provides justification for other values and is not justified by any other value. A distinction between final intrinsic and final extrinsic value in terms of supervenience is subsequently introduced. Section 2.2 contains a discussion of the debate about instrumental value and other varieties of nonfinal extrinsic value. Finally, section 2.3 focuses on recent attacks on the very coherence of the intrinsic/extrinsic distinction and its role as a demarcation line between fundamental and nonfundamental value."
10.48550/arXiv.2212.04960,BigScience: A Case Study in the Social Construction of a Multilingual Large Language Model,Pistilli G.,"The BigScience Workshop was a value-driven initiative that spanned one and half years of interdisciplinary research and culminated in the creation of ROOTS, a 1.6TB multilingual dataset that was used to train BLOOM, one of the largest multilingual language models to date. In addition to the technical outcomes and artifacts, the workshop fostered multidisciplinary collaborations around large models, datasets, and their analysis. This in turn led to a wide range of research publications spanning topics from ethics to law, data governance, modeling choices and distributed training. This paper focuses on the collaborative research aspects of BigScience and takes a step back to look at the challenges of large-scale participatory research, with respect to participant diversity and the tasks required to successfully carry out such a project. Our main goal is to share the lessons we learned from this experience, what we could have done better and what we did well. We show how the impact of such a social approach to scientific research goes well beyond the technical artifacts that were the basis of its inception."
10.1002/9781118922590.ch26,The Paradoxes of Time Travel,Lewis D.,"The paradoxes of time travel are oddities, not impossibilities. This chapter concerns with the sort of time travel that is recounted in science fiction. It argues that what goes on in a time travel story may be a possible pattern of events in four-dimensional space-time with no extra time dimension; that it may be correct to regard the scattered stages of the alleged time traveler as comprising a single person; and that we may legitimately assign to those stages and their surroundings a personal time order that disagrees sometimes with their order in external time. Some might concede all this, but protest that the impossibility of time travel is revealed after all when we ask not what the time traveler does, but what he could do. Finally the chapter provides time travel story about Tim and Tom."
10.1111/j.1747-9991.2007.00063.x,Mental Causation,Bennett K.,"Concerns about ‘mental causation’ are concerns about how it is possible for mental states to cause anything to happen. How does what we believe, want, see, feel, hope, or dread manage to cause us to act? Certain positions on the mind-body problem – including some forms of physicalism – make such causation look highly problematic. This entry sketches several of the main reasons to worry, and raises some questions for further investigation."